/**
 * Chunk Processor Module
 * 
 * Handles processing of individual PDF chunks:
 * - Initialize chunk state
 * - Execute workflow for chunk
 * - Extract and return chunk results
 * 
 * NEW: Integrated with smart image assignment system
 * - AI analyzes each page â†’ PageMetadata
 * - Pages inserted into PageRegistry
 * - Real-time image assignment
 */

import { buildEnhancedWorkflow } from '../graph-enhanced';
import type { State } from '../state';
import type { PdfChunk } from '../../utils/pdf/chunker';
import { analyzePageWithAI } from '../agents/page-analyzer.agent';
import { PageMetadata } from '../types/page-metadata';
import { pdfToImages } from '../../utils/pdf/converter';
import { join } from 'path';
import { renameSync } from 'fs';
import { uploadFileToPdfCacheWithVariants } from '../../services/r2-storage';

export interface ChunkProcessingConfig {
  chunk: PdfChunk & { sourceFile: string; pdfHash: string };
  chunkIndex: number;
  totalChunks: number;
  outputDir: string;
  jobId?: string; // For progress tracking
}

export interface ChunkProcessingResult {
  success: boolean;
  chunkIndex: number;
  data: any | null;
  errors: string[];
  warnings: string[];
  processingTime?: number;
  pageRange?: { start: number; end: number };
  pageResults?: any[];  // Detailed page-level results
  pageMetadataList?: PageMetadata[];  // NEW: AI analyzed page metadata
}

/**
 * Process a single PDF chunk with smart image assignment
 */
export async function processSingleChunk(
  config: ChunkProcessingConfig
): Promise<ChunkProcessingResult> {
  const { chunk, chunkIndex, totalChunks, outputDir, jobId } = config;
  const startTime = Date.now();

  console.log(`   ğŸš€ Chunk ${chunkIndex + 1}/${totalChunks}: é¡µ ${chunk.pageRange.start}-${chunk.pageRange.end}`);

  try {
    // 1. è½¬æ¢chunkä¸ºå›¾ç‰‡
    const pageImages = await convertChunkToImages(chunk, outputDir, jobId, chunkIndex);
    
    // 2. AIåˆ†ææ¯ä¸€é¡µ
    const pageMetadataList = await Promise.all(
      pageImages.map(async (pageImgPath, localIdx) => {
        const absolutePageNum = chunk.pageRange.start + localIdx;
        return await analyzePageWithAI(
          pageImgPath,           // æœ¬åœ°è·¯å¾„ï¼ˆAIåˆ†æç”¨ï¼‰
          absolutePageNum,
          chunk.sourceFile,
          chunkIndex,
          jobId
        );
      })
    );
    
    console.log(`   âœ“ AI analyzed ${pageMetadataList.length} pages`);
    
    // âš¡ PERFORMANCE: Batch upload all images to R2 PDF cache after analysis
    // â­ NEW: Use PDF hash for cache key, enabling image reuse across uploads
    const pdfHash = chunk.pdfHash;
    if (!pdfHash) {
      throw new Error('âŒ PDF hash is required for R2 upload');
    }
    
    console.log(`   ğŸ“¤ Batch uploading ${pageImages.length} images to R2 cache (${pdfHash.substring(0, 12)}...)...`);
    const uploadStartTime = Date.now();
    
    // âš¡ Upload images with controlled concurrency to avoid R2 rate limits
    // Upload in smaller batches (3 at a time) instead of all at once
    const uploadResults: any[] = [];
    const UPLOAD_CONCURRENCY = 3; // âš¡ Max 3 concurrent uploads per chunk
    
    for (let i = 0; i < pageImages.length; i += UPLOAD_CONCURRENCY) {
      const batch = pageImages.slice(i, i + UPLOAD_CONCURRENCY);
      
      const batchResults = await Promise.allSettled(
        batch.map(async (imgPath, batchIdx) => {
          const idx = i + batchIdx;
          
          // âš¡ Small delay between uploads in the same batch
          await new Promise(resolve => setTimeout(resolve, batchIdx * 200));
          
          let retries = 3;
          let lastError: any;
          
          // Retry logic for R2 upload to PDF cache (with variants)
          while (retries > 0) {
            try {
              const imageUrls = await uploadFileToPdfCacheWithVariants(imgPath, pdfHash);
              
              // âš¡ Update imagePath and imageUrls in pageMetadataList
              if (pageMetadataList[idx]?.images) {
                pageMetadataList[idx].images.forEach(img => {
                  img.imagePath = imageUrls.original;  // å‘åå…¼å®¹
                  img.imageUrls = imageUrls;           // â­ å¤šå°ºå¯¸URLs
                });
              }
              
              return { success: true, path: imgPath, url: imageUrls.original };
            } catch (err) {
              lastError = err;
              retries--;
              
              if (retries > 0) {
                console.warn(`   âš ï¸ Upload failed for image ${idx + 1}, retrying... (${retries} attempts left)`);
                await new Promise(resolve => setTimeout(resolve, 1500)); // âš¡ Increased retry delay
              }
            }
          }
          
          // All retries failed
          console.error(`   âŒ Failed to upload image ${idx + 1} after 3 attempts:`, lastError);
          return { success: false, path: imgPath, error: lastError };
        })
      );
      
      uploadResults.push(...batchResults);
      
      // âš¡ Delay between batches to avoid overwhelming R2
      if (i + UPLOAD_CONCURRENCY < pageImages.length) {
        await new Promise(resolve => setTimeout(resolve, 300));
      }
    }
    
    const successCount = uploadResults.filter(r => r.status === 'fulfilled' && r.value.success).length;
    const failedCount = pageImages.length - successCount;
    const uploadTime = Date.now() - uploadStartTime;
    
    // â­ Track how many were actually uploaded vs reused from cache
    const uploadedCount = uploadResults.filter(r => 
      r.status === 'fulfilled' && 
      r.value.success && 
      !String(r.value.url).includes('already cached')
    ).length;
    const cachedCount = successCount - uploadedCount;
    
    if (cachedCount > 0) {
      console.log(`   ğŸ“Š Upload results: ${successCount}/${pageImages.length} succeeded in ${uploadTime}ms`);
      console.log(`   â™»ï¸  Cache reuse: ${cachedCount} images already existed, skipped upload`);
      console.log(`   â¬†ï¸  New uploads: ${uploadedCount} images uploaded`);
    } else {
      console.log(`   ğŸ“Š Upload results: ${successCount}/${pageImages.length} succeeded in ${uploadTime}ms`);
    }
    
    // âš ï¸ If any uploads failed, log detailed error but continue processing
    if (failedCount > 0) {
      console.error(`   âš ï¸ WARNING: ${failedCount} images failed to upload to R2`);
      console.error(`   These images will not be accessible in the frontend.`);
      console.error(`   Please check R2 configuration and network connectivity.`);
      
      // Log failed uploads
      uploadResults.forEach((result, idx) => {
        if (result.status === 'fulfilled' && !result.value.success) {
          console.error(`     - Image ${idx + 1}: ${result.value.path}`);
        }
      });
    }

    // âš¡ PERFORMANCE FIX: Only run full workflow on first and last chunk
    // Other chunks only do page-level analysis
    let chunkResult: any;
    
    if (chunkIndex === 0 || chunkIndex === totalChunks - 1) {
      console.log(`   ğŸ”„ Running full workflow for chunk ${chunkIndex + 1} (first/last chunk)`);
      
      // 3. æ‰§è¡Œå®Œæ•´workflowï¼ˆæå–é¡¹ç›®åŸºæœ¬ä¿¡æ¯ã€å•å…ƒä¿¡æ¯ç­‰ï¼‰
      const initialState: Partial<State> = {
        pdfPath: `${chunk.sourceFile}_chunk${chunkIndex + 1}`,
        pdfBuffer: chunk.buffer,
        outputDir: outputDir,
        buildingData: {},
        totalPages: 0,
        pageImages: pageImages,  // âš¡ Pass converted images to avoid re-conversion
        pageResults: [],
        categorizedImages: {
          cover: [],
          renderings: [],
          floorPlans: [],
          amenities: [],
          maps: [],
        },
        retryCount: 0,
        errors: [],
        warnings: [],
        startTime,
        processingStage: 'ingestion',
      };

      const app = buildEnhancedWorkflow();
      chunkResult = await app.invoke(initialState);
    } else {
      console.log(`   âš¡ Skipping full workflow for chunk ${chunkIndex + 1} (only doing page analysis)`);
      
      // Skip expensive workflow for middle chunks
      chunkResult = {
        buildingData: {},
        errors: [],
        warnings: [],
        pageResults: [],
      };
    }
    
    const processingTime = Date.now() - startTime;
    const unitsCount = chunkResult.buildingData?.units?.length || 0;
    
    console.log(`   âœ“ Chunk ${chunkIndex + 1} complete: ${unitsCount} units, ${pageMetadataList.length} pages analyzed (${(processingTime / 1000).toFixed(2)}s)`);

    return {
      success: true,
      chunkIndex,
      data: chunkResult.buildingData,
      errors: chunkResult.errors || [],
      warnings: chunkResult.warnings || [],
      processingTime,
      pageRange: chunk.pageRange,
      pageResults: chunkResult.pageResults || [],
      pageMetadataList,
    };
  } catch (error) {
    console.error(`   âœ— Chunk ${chunkIndex + 1} failed:`, error);
    
    return {
      success: false,
      chunkIndex,
      data: null,
      errors: [`Chunk ${chunkIndex + 1} failed: ${error}`],
      warnings: [],
      processingTime: Date.now() - startTime,
      pageRange: chunk.pageRange,
    };
  }
}

/**
 * è½¬æ¢chunkä¸ºå›¾ç‰‡ï¼ˆè¾…åŠ©å‡½æ•°ï¼‰
 * 
 * â­ KEY FIX: Use absolute page numbers for filename prefix
 * This ensures:
 * - Same PDF always generates same filenames (cache-friendly)
 * - Clear page identification (page_1.png = page 1 of PDF)
 * - Independent of chunk size changes
 * 
 * å…³é”®ä¿®å¤ï¼š
 * - ä½¿ç”¨PDFç»å¯¹é¡µç ä½œä¸ºæ–‡ä»¶åå‰ç¼€
 * - è¿”å›æœ¬åœ°è·¯å¾„ä¾›AIåˆ†æï¼ˆreadFileSyncéœ€è¦ï¼‰
 * - R2ä¸Šä¼ åœ¨PageImageä¸­å¤„ç†
 */
async function convertChunkToImages(
  chunk: PdfChunk & { sourceFile: string },
  outputDir: string,
  _jobId: string | undefined,  // Unused for now, kept for future use
  chunkIndex: number
): Promise<string[]> {
  const tempDir = join(outputDir, `chunk_${chunkIndex}_temp`);
  
  // â­ Use absolute page number as prefix (not chunk index!)
  // Example: Chunk 0 (pages 1-5) â†’ page_1, page_2, page_3, page_4, page_5
  // Example: Chunk 1 (pages 6-10) â†’ page_6, page_7, page_8, page_9, page_10
  const firstPageInChunk = chunk.pageRange.start;
  const filenamePrefix = `page_${firstPageInChunk}`;
  
  // è½¬æ¢PDF chunkä¸ºå›¾ç‰‡
  const localImagePaths = await pdfToImages(
    chunk.buffer,
    tempDir,
    filenamePrefix
  );
  
  // âš ï¸ IMPORTANT: pdfToImages generates sequential numbers starting from .1
  // So we need to rename files to use absolute page numbers
  // page_1.1.png â†’ page_1.png, page_1.2.png â†’ page_2.png, etc.
  const renamedPaths: string[] = [];
  for (let i = 0; i < localImagePaths.length; i++) {
    const oldPath = localImagePaths[i];
    const absolutePageNum = firstPageInChunk + i;
    const newPath = join(tempDir, `page_${absolutePageNum}.png`);
    
    // Rename file to use absolute page number
    renameSync(oldPath, newPath);
    renamedPaths.push(newPath);
  }
  
  // â­ è¿”å›æœ¬åœ°è·¯å¾„ï¼ˆAIåˆ†æéœ€è¦ï¼‰
  // R2ä¸Šä¼ ç¨ååœ¨PageImageä¸­å¤„ç†
  return renamedPaths;
}

