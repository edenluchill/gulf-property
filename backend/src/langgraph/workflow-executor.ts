/**
 * PDF Workflow Executor
 * 
 * Main entry point for executing PDF processing workflow.
 * 
 * Strategy:
 * 1. Split all PDFs into uniform chunks (default: 5 pages per chunk)
 * 2. Process chunks in parallel batches (with rate limiting)
 * 3. Aggregate and merge results in real-time
 * 4. Return finalized building data
 * 
 * Perfect for processing single or multiple PDFs of any size!
 */

import { join } from 'path';
import { createOutputStructure } from '../utils/pdf/file-manager';
import { progressEmitter } from '../services/progress-emitter';
import { splitAllPdfsIntoChunks } from './strategies/chunking';
import { processChunksInBatches } from './processors/batch-processor';
import { 
  createEmptyAggregatedData
} from './processors/data-aggregator';
import { ResultRecorder } from './processors/result-recorder';
import { calculatePdfHashes, shortHash } from '../utils/pdf/pdf-hash';
import { checkPdfCache } from '../services/r2-storage';

export interface WorkflowConfig {
  pdfBuffers: Buffer[];
  pdfNames?: string[];
  outputBaseDir?: string;
  jobId: string;
  pagesPerChunk?: number;
  batchSize?: number;
  batchDelay?: number;
}

export interface WorkflowResult {
  success: boolean;
  buildingData: any;
  processingTime: number;
  errors: string[];
  warnings: string[];
  jobId: string;
  totalChunks: number;
  totalPages: number;
  analysisReportPath?: string;  // Path to detailed JSON analysis report
}

/**
 * Execute PDF processing workflow
 * 
 * @param config - Workflow configuration
 * @returns Processing result with building data
 */
export async function executePdfWorkflow(
  config: WorkflowConfig
): Promise<WorkflowResult> {
  const startTime = Date.now();
  const { 
    jobId, 
    pdfBuffers, 
    pdfNames, 
    pagesPerChunk = 5,
    batchSize = 10,
    batchDelay = 1000,
  } = config;

  console.log(`\n${'='.repeat(70)}`);
  console.log(`üìã PDF PROCESSING WORKFLOW STARTED`);
  console.log(`   Job ID: ${jobId}`);
  console.log(`   Files: ${pdfBuffers.length} | Pages per chunk: ${pagesPerChunk}`);
  console.log(`   Batch size: ${batchSize} | Batch delay: ${batchDelay}ms`);
  console.log(`   PDF sizes: ${pdfBuffers.map(b => `${(b.length / 1024).toFixed(2)} KB`).join(', ')}`);
  console.log(`${'='.repeat(70)}\n`);

  try {
    console.log(`‚öôÔ∏è Workflow execution starting for job ${jobId}...`);
    
    // ============================================================
    // STEP 0: Calculate PDF hashes and check cache
    // ============================================================
    console.log(`\nüîê Calculating PDF hashes for cache lookup...`);
    const pdfHashes = calculatePdfHashes(pdfBuffers, pdfNames);
    
    // Check if we have cached results for any PDFs
    progressEmitter.emit(jobId, {
      stage: 'ingestion',
      message: 'üîç Ê£ÄÊü•PDFÁºìÂ≠ò...',
      progress: 2,
      timestamp: Date.now(),
    });
    
    const cacheResults = await Promise.all(
      pdfHashes.map(async ({ hash, name }) => {
        const cached = await checkPdfCache(hash);
        if (cached) {
          console.log(`   ‚úÖ Cache HIT for "${name}" (${shortHash(hash)})`);
        } else {
          console.log(`   ‚ö†Ô∏è  Cache MISS for "${name}" (${shortHash(hash)})`);
        }
        return { hash, name, cached };
      })
    );
    
    const allCached = cacheResults.every(r => r.cached !== null);
    
    if (allCached) {
      console.log(`\nüéâ All PDFs found in cache! Skipping processing...`);
      progressEmitter.emit(jobId, {
        stage: 'ingestion',
        message: '‚úÖ ‰ΩøÁî®ÁºìÂ≠òÊï∞ÊçÆÔºàÁßíÁ∫ßËøîÂõûÔºâ',
        progress: 90,
        timestamp: Date.now(),
      });
      
      // TODO: Reconstruct buildingData from cached images
      // For now, we still process but will reuse images
      console.log(`   ‚ÑπÔ∏è  Note: Full cache reconstruction not yet implemented`);
      console.log(`   ‚ÑπÔ∏è  Will process but reuse cached images`);
    }
    
    // Setup output directory
    const outputBaseDir = config.outputBaseDir || join(process.cwd(), 'uploads', 'langgraph-output');
    const outputStructure = createOutputStructure(outputBaseDir, jobId);

    // Initialize result recorder for detailed analysis
    const resultRecorder = new ResultRecorder(
      jobId, 
      outputStructure.jobDir,
      pdfNames || pdfBuffers.map((_, i) => `Document ${i + 1}`)
    );
    
    resultRecorder.setMetadata({
      pagesPerChunk,
      batchSize,
      batchDelay,
    });

    // ============================================================
    // STEP 1: Split all PDFs into chunks
    // ============================================================
    progressEmitter.emit(jobId, {
      stage: 'ingestion',
      message: `üì¶ Ê≠£Âú®Â∞ÜÊâÄÊúâÊñáÊ°£ÂàáÂàÜÊàê ${pagesPerChunk} È°µÂ∞èÂùó...`,
      progress: 5,
      timestamp: Date.now(),
    });

    const { chunks, totalChunks, totalPages } = await splitAllPdfsIntoChunks({
      pdfBuffers,
      pdfNames,
      pdfHashes: pdfHashes.map(h => h.hash),  // ‚≠ê Pass PDF hashes for caching
      pagesPerChunk,
    });

    progressEmitter.emit(jobId, {
      stage: 'ingestion',
      message: `‚úÖ Â∑≤ÂàáÂàÜÊàê ${totalChunks} ‰∏™Â∞èÂùóÔºåÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ...`,
      progress: 10,
      timestamp: Date.now(),
    });

    // ============================================================
    // STEP 2: Process chunks in batches and aggregate data
    // ============================================================
    console.log(`\nüîÑ Starting batch processing of ${totalChunks} chunks...`);
    
    const aggregatedData = createEmptyAggregatedData();

    let allErrors: string[] = [];
    let allWarnings: string[] = [];
    let chunkAnalyses: any[] = [];
    let finalAggregatedData = aggregatedData;

    try {
      const batchResult = await processChunksInBatches(
        {
          chunks,
          outputDir: outputStructure.jobDir,
          jobId,
          batchSize,
          batchDelay,
        },
        aggregatedData
      );
      
      allErrors = batchResult.allErrors;
      allWarnings = batchResult.allWarnings;
      chunkAnalyses = batchResult.chunkAnalyses;
      finalAggregatedData = batchResult.aggregatedData;
      
      console.log(`‚úÖ Batch processing completed successfully`);
    } catch (batchError) {
      console.error(`‚ùå Batch processing failed:`, batchError);
      // Re-throw to be caught by outer try-catch
      throw new Error(`Batch processing failed: ${batchError}`);
    }
    
    // Record all chunk analyses
    console.log(`üìù Recording ${chunkAnalyses.length} chunk analyses...`);
    chunkAnalyses.forEach(chunk => resultRecorder.recordChunk(chunk));

    // ============================================================
    // STEP 3: ÊúÄÁªàÊï∞ÊçÆÂ∑≤Âú®PageRegistry‰∏≠ÔºåÁõ¥Êé•‰ΩøÁî®
    // ============================================================
    console.log(`\nüìä Processing complete. Using smart assignment result...`);

    progressEmitter.emit(jobId, {
      stage: 'reducing',
      message: '‚úÖ Êô∫ËÉΩÂàÜÈÖçÂÆåÊàê',
      progress: 95,
      timestamp: Date.now(),
    });

    // ‚≠ê ‰ΩøÁî®Êô∫ËÉΩÂàÜÈÖçÁ≥ªÁªüÁöÑÊúÄÁªàÁªìÊûúÔºàÂ∑≤Âú®batch-processor‰∏≠ËΩ¨Êç¢Ôºâ
    const finalData = finalAggregatedData;

    // Log summary
    console.log(`\nüìä Final Summary:`);
    console.log(`   Units: ${aggregatedData.units.length} ‚Üí ${finalData.units.length} (deduplicated)`);
    console.log(`   Payment plans: ${finalData.paymentPlans.length}`);
    console.log(`   Amenities: ${finalData.amenities.length}`);
    console.log(`   Images: ${finalData.images.projectImages.length} project, ${finalData.images.floorPlanImages.length} floor plans`);

    // ============================================================
    // STEP 4: Save detailed analysis report
    // ============================================================
    const analysisReportPath = resultRecorder.saveFullReport(
      finalData,
      allErrors,
      allWarnings
    );

    // ============================================================
    // STEP 5: Return result
    // ============================================================
    const processingTime = Date.now() - startTime;

    console.log(`\n${'='.repeat(70)}`);
    console.log('‚úÖ WORKFLOW COMPLETED');
    console.log(`Time: ${(processingTime / 1000).toFixed(2)}s | Chunks: ${totalChunks} | Pages: ${totalPages}`);
    console.log(`${'='.repeat(70)}\n`);

    return {
      success: allErrors.length === 0,
      buildingData: finalData,
      processingTime,
      errors: allErrors,
      warnings: allWarnings,
      jobId,
      totalChunks,
      totalPages,
      analysisReportPath,
    };

  } catch (error) {
    console.error('\n‚ùå WORKFLOW FAILED:', error);
    console.error('   Stack trace:', (error as Error).stack);
    
    const errorMessage = error instanceof Error ? error.message : String(error);
    
    // Try to emit error to client
    try {
      progressEmitter.error(jobId, errorMessage);
    } catch (emitError) {
      console.error('   Failed to emit error to client:', emitError);
    }

    return {
      success: false,
      buildingData: {},
      processingTime: Date.now() - startTime,
      errors: [errorMessage],
      warnings: [],
      jobId,
      totalChunks: 0,
      totalPages: 0,
      analysisReportPath: undefined,
    };
  }
}
